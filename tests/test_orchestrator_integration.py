"""Integration tests for refinement + story sprint orchestration."""

from __future__ import annotations

import json
from pathlib import Path

import pytest

from automated_software_developer.agent.orchestrator import AgentConfig, SoftwareDevelopmentAgent
from automated_software_developer.agent.providers.mock_provider import MockProvider


def _verification_command() -> str:
    return (
        'python -c "from pathlib import Path; '
        "assert Path('artifact.txt').read_text(encoding='utf-8').strip() == 'ok'\""
    )


def _refinement_payload() -> dict[str, object]:
    return {
        "project_name": "Artifact Project",
        "product_brief": "Create and validate an artifact file.",
        "personas": ["Developer"],
        "stories": [
            {
                "id": "story-artifact",
                "title": "Create artifact file",
                "story": (
                    "As a developer, I want artifact output so that checks can validate behavior."
                ),
                "acceptance_criteria": [
                    "Given output generation, when run completes, then artifact.txt contains ok"
                ],
                "nfr_tags": ["reliability"],
                "dependencies": [],
                "verification_commands": [_verification_command()],
            }
        ],
        "nfrs": {
            "security": ["Avoid writing secrets to files."],
            "privacy": [],
            "performance": [],
            "reliability": ["Verification commands must pass."],
            "observability": [],
            "ux_accessibility": [],
            "compliance": [],
        },
        "ambiguities": [],
        "contradictions": [],
        "missing_constraints": [],
        "edge_cases": [],
        "external_dependencies": [],
        "assumptions": [
            {
                "assumption": "Artifact content should be exactly ok.",
                "testable_criterion": (
                    "Given validation executes, when artifact.txt is read, then content equals ok."
                ),
            }
        ],
        "stack_rationale": "Python is concise and testable for scripted automation.",
        "global_verification_commands": [_verification_command()],
    }


def _architecture_payload() -> dict[str, object]:
    return {
        "overview": "The system is split into an API, storage, and validation layer.",
        "components": [
            {
                "id": "api",
                "name": "API Service",
                "responsibilities": ["Expose endpoints", "Coordinate operations"],
                "interfaces": ["REST"],
                "dependencies": ["storage", "validation"],
            },
            {
                "id": "storage",
                "name": "Storage Layer",
                "responsibilities": ["Persist artifacts"],
                "interfaces": ["Filesystem"],
                "dependencies": [],
            },
            {
                "id": "validation",
                "name": "Validation Service",
                "responsibilities": ["Validate outputs"],
                "interfaces": ["Python API"],
                "dependencies": ["storage"],
            },
        ],
        "adrs": [
            {
                "id": "adr-001",
                "title": "Filesystem storage",
                "status": "accepted",
                "context": "Artifacts are lightweight and local.",
                "decision": "Use filesystem storage for outputs.",
                "consequences": ["Simpler operations", "Local storage constraints"],
            }
        ],
    }


def test_orchestrator_story_retry_and_artifacts(tmp_path: Path) -> None:
    provider = MockProvider(
        responses=[
            _refinement_payload(),
            _architecture_payload(),
            {
                "summary": "Initial implementation writes wrong artifact content.",
                "operations": [
                    {"op": "write_file", "path": "artifact.txt", "content": "bad\n"},
                ],
                "verification_commands": [],
            },
            {
                "summary": "Fix artifact content and add README.",
                "operations": [
                    {"op": "write_file", "path": "artifact.txt", "content": "ok\n"},
                    {
                        "op": "write_file",
                        "path": "README.md",
                        "content": "# Artifact Project\n\nGenerated by test fixture.\n",
                    },
                ],
                "verification_commands": [],
            },
        ]
    )
    agent = SoftwareDevelopmentAgent(provider=provider, config=AgentConfig(max_task_attempts=3))
    summary = agent.run(requirements="Create an artifact file containing ok", output_dir=tmp_path)

    assert summary.tasks_completed == 1
    assert (tmp_path / "artifact.txt").read_text(encoding="utf-8").strip() == "ok"
    assert (tmp_path / "README.md").exists()
    assert (tmp_path / ".autosd" / "refined_requirements.md").exists()
    assert (tmp_path / ".autosd" / "backlog.json").exists()
    assert (tmp_path / ".autosd" / "design_doc.md").exists()
    assert (tmp_path / ".autosd" / "platform_plan.json").exists()
    assert (tmp_path / ".autosd" / "capability_graph.json").exists()
    assert (tmp_path / ".autosd" / "architecture" / "architecture.md").exists()
    assert (tmp_path / ".autosd" / "architecture" / "components.json").exists()
    assert (tmp_path / ".autosd" / "architecture" / "adrs" / "adr-001.md").exists()
    assert (tmp_path / ".autosd" / "provenance" / "build_manifest.json").exists()
    assert (tmp_path / ".autosd" / "provenance" / "build_hash.json").exists()
    assert (tmp_path / ".autosd" / "prompt_journal.jsonl").exists()
    assert (tmp_path / ".autosd" / "sprint_log.jsonl").exists()

    backlog = json.loads((tmp_path / ".autosd" / "backlog.json").read_text(encoding="utf-8"))
    assert backlog["stories"][0]["status"] == "completed"
    assert backlog["stories"][0]["attempts"] == 2

    progress = json.loads((tmp_path / ".autosd" / "progress.json").read_text(encoding="utf-8"))
    assert progress["tasks"][0]["status"] == "completed"
    assert progress["tasks"][0]["attempts"] == 2
    assert progress["design_doc"] == ".autosd/design_doc.md"
    assert progress["platform_plan"] == ".autosd/platform_plan.json"
    assert progress["capability_graph"] == ".autosd/capability_graph.json"
    assert progress["architecture_doc"] == ".autosd/architecture/architecture.md"
    assert progress["architecture_components"] == ".autosd/architecture/components.json"
    assert progress["architecture_adrs"] == ".autosd/architecture/adrs"
    assert isinstance(progress["platform_adapter_id"], str)

    journal_lines = (
        (tmp_path / ".autosd" / "prompt_journal.jsonl").read_text(encoding="utf-8").splitlines()
    )
    assert len(journal_lines) == 3
    outcomes = [json.loads(line).get("outcome") for line in journal_lines]
    assert outcomes.count("pass") == 2
    assert outcomes.count("fail") == 1


def test_agent_config_rejects_invalid_values() -> None:
    with pytest.raises(ValueError):
        AgentConfig(max_task_attempts=0)
    with pytest.raises(ValueError):
        AgentConfig(max_stories_per_sprint=0)
    with pytest.raises(ValueError):
        AgentConfig(command_timeout_seconds=0)
    with pytest.raises(ValueError):
        AgentConfig(security_scan_mode="invalid")
